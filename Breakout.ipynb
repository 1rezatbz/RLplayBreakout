{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugH5WqElSnt6",
        "outputId": "1439702a-f2bc-4734-9423-2556ed7c1af3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:2 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Fetched 163 kB in 2s (73.1 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "47 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "swig is already the newest version (3.0.12-1).\n",
            "libpq-dev is already the newest version (10.19-0ubuntu0.18.04.1).\n",
            "xorg-dev is already the newest version (1:7.7+19ubuntu7.1).\n",
            "libsdl2-dev is already the newest version (2.0.8+dfsg1-1ubuntu1.18.04.4).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.10).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.\n",
            "Requirement already satisfied: tf-agents in /usr/local/lib/python3.7/dist-packages (0.12.1)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (3.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.17.3)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.21.6)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.14.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (4.2.0)\n",
            "Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.23.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from tf-agents) (7.1.2)\n",
            "Requirement already satisfied: tensorflow-probability>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.16.0)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.5.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (4.11.3)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (0.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym>=0.17.0->tf-agents) (3.8.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents) (0.1.7)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents) (0.5.3)\n",
            "Requirement already satisfied: gym[accept-rom-license,atari,box2d] in /usr/local/lib/python3.7/dist-packages (0.23.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari,box2d]) (0.0.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari,box2d]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari,box2d]) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari,box2d]) (1.21.6)\n",
            "Requirement already satisfied: ale-py~=0.7.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari,box2d]) (0.7.5)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari,box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari,box2d]) (2.1.0)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari,box2d]) (0.4.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.4->gym[accept-rom-license,atari,box2d]) (5.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (4.64.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (0.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym[accept-rom-license,atari,box2d]) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym[accept-rom-license,atari,box2d]) (4.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (2.10)\n"
          ]
        }
      ],
      "source": [
        "# Python version\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# In case colab is being used\n",
        "gColab = \"google.colab\" in sys.modules\n",
        "\n",
        "if gColab:\n",
        "    !apt update && apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb\n",
        "    %pip install -U tf-agents pyvirtualdisplay\n",
        "    %pip install -U gym>=0.21.0\n",
        "    %pip install -U gym[box2d,atari,accept-rom-license]\n",
        "\n",
        "# Scikit-Learn version\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# TensorFlow version\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. Training will be slow.\")\n",
        "    if gColab:\n",
        "        print(\"Change runtime to GPU to accelerate performance.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sbalm6wWStcu",
        "outputId": "b4ef42c2-1df7-4b78-9795-b2de661ee7fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 0\n",
            "\t\t EnvironmentSteps = 0\n",
            "\t\t AverageReturn = 0.0\n",
            "\t\t AverageEpisodeLength = 0.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000/10000"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 0\n",
            "\t\t EnvironmentSteps = 4\n",
            "\t\t AverageReturn = 0.0\n",
            "\t\t AverageEpisodeLength = 0.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "998 loss:0.00734"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 26\n",
            "\t\t EnvironmentSteps = 4004\n",
            "\t\t AverageReturn = 0.8999999761581421\n",
            "\t\t AverageEpisodeLength = 154.3000030517578\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1995 loss:0.00086"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 48\n",
            "\t\t EnvironmentSteps = 8004\n",
            "\t\t AverageReturn = 1.0\n",
            "\t\t AverageEpisodeLength = 174.1999969482422\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2995 loss:0.00011"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 73\n",
            "\t\t EnvironmentSteps = 12004\n",
            "\t\t AverageReturn = 1.5\n",
            "\t\t AverageEpisodeLength = 180.3000030517578\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3999 loss:0.00012"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 97\n",
            "\t\t EnvironmentSteps = 16004\n",
            "\t\t AverageReturn = 1.2999999523162842\n",
            "\t\t AverageEpisodeLength = 172.3000030517578\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4997 loss:0.00012"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 120\n",
            "\t\t EnvironmentSteps = 20004\n",
            "\t\t AverageReturn = 1.600000023841858\n",
            "\t\t AverageEpisodeLength = 196.8000030517578\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5999 loss:0.00005"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 146\n",
            "\t\t EnvironmentSteps = 24004\n",
            "\t\t AverageReturn = 0.6000000238418579\n",
            "\t\t AverageEpisodeLength = 139.60000610351562\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6998 loss:0.00047"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 169\n",
            "\t\t EnvironmentSteps = 28004\n",
            "\t\t AverageReturn = 0.8999999761581421\n",
            "\t\t AverageEpisodeLength = 176.1999969482422\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7995 loss:0.00012"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 192\n",
            "\t\t EnvironmentSteps = 32004\n",
            "\t\t AverageReturn = 0.6000000238418579\n",
            "\t\t AverageEpisodeLength = 196.89999389648438\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8998 loss:0.00009"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 215\n",
            "\t\t EnvironmentSteps = 36004\n",
            "\t\t AverageReturn = 1.0\n",
            "\t\t AverageEpisodeLength = 201.3000030517578\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9998 loss:0.00024"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 237\n",
            "\t\t EnvironmentSteps = 40004\n",
            "\t\t AverageReturn = 1.2000000476837158\n",
            "\t\t AverageEpisodeLength = 206.6999969482422\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10995 loss:0.00030"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 257\n",
            "\t\t EnvironmentSteps = 44004\n",
            "\t\t AverageReturn = 0.800000011920929\n",
            "\t\t AverageEpisodeLength = 197.1999969482422\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11997 loss:0.00026"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 280\n",
            "\t\t EnvironmentSteps = 48004\n",
            "\t\t AverageReturn = 0.800000011920929\n",
            "\t\t AverageEpisodeLength = 167.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12996 loss:0.00009"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 303\n",
            "\t\t EnvironmentSteps = 52004\n",
            "\t\t AverageReturn = 0.699999988079071\n",
            "\t\t AverageEpisodeLength = 167.3000030517578\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13999 loss:0.00012"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 325\n",
            "\t\t EnvironmentSteps = 56004\n",
            "\t\t AverageReturn = 0.8999999761581421\n",
            "\t\t AverageEpisodeLength = 179.10000610351562\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14997 loss:0.00005"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 346\n",
            "\t\t EnvironmentSteps = 60004\n",
            "\t\t AverageReturn = 1.5\n",
            "\t\t AverageEpisodeLength = 195.5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15997 loss:0.00014"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 370\n",
            "\t\t EnvironmentSteps = 64004\n",
            "\t\t AverageReturn = 0.8999999761581421\n",
            "\t\t AverageEpisodeLength = 186.3000030517578\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16995 loss:0.00005"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 397\n",
            "\t\t EnvironmentSteps = 68004\n",
            "\t\t AverageReturn = 0.8999999761581421\n",
            "\t\t AverageEpisodeLength = 149.8000030517578\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17999 loss:0.00012"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 418\n",
            "\t\t EnvironmentSteps = 72004\n",
            "\t\t AverageReturn = 0.800000011920929\n",
            "\t\t AverageEpisodeLength = 175.8000030517578\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18998 loss:0.00011"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 440\n",
            "\t\t EnvironmentSteps = 76004\n",
            "\t\t AverageReturn = 0.800000011920929\n",
            "\t\t AverageEpisodeLength = 152.8000030517578\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19997 loss:0.00004"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 466\n",
            "\t\t EnvironmentSteps = 80004\n",
            "\t\t AverageReturn = 1.2999999523162842\n",
            "\t\t AverageEpisodeLength = 170.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20995 loss:0.00009"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 490\n",
            "\t\t EnvironmentSteps = 84004\n",
            "\t\t AverageReturn = 1.0\n",
            "\t\t AverageEpisodeLength = 174.8000030517578\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21998 loss:0.00010"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 514\n",
            "\t\t EnvironmentSteps = 88004\n",
            "\t\t AverageReturn = 1.2999999523162842\n",
            "\t\t AverageEpisodeLength = 167.5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22997 loss:0.00012"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 539\n",
            "\t\t EnvironmentSteps = 92004\n",
            "\t\t AverageReturn = 0.8999999761581421\n",
            "\t\t AverageEpisodeLength = 150.10000610351562\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23997 loss:0.00048"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 559\n",
            "\t\t EnvironmentSteps = 96004\n",
            "\t\t AverageReturn = 1.100000023841858\n",
            "\t\t AverageEpisodeLength = 178.8000030517578\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24999 loss:0.00009"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 581\n",
            "\t\t EnvironmentSteps = 100004\n",
            "\t\t AverageReturn = 2.0999999046325684\n",
            "\t\t AverageEpisodeLength = 199.89999389648438\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25998 loss:0.00007"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 602\n",
            "\t\t EnvironmentSteps = 104004\n",
            "\t\t AverageReturn = 0.8999999761581421\n",
            "\t\t AverageEpisodeLength = 171.8000030517578\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26999 loss:0.00008"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 627\n",
            "\t\t EnvironmentSteps = 108004\n",
            "\t\t AverageReturn = 1.600000023841858\n",
            "\t\t AverageEpisodeLength = 194.8000030517578\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27999 loss:0.00014"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 651\n",
            "\t\t EnvironmentSteps = 112004\n",
            "\t\t AverageReturn = 0.699999988079071\n",
            "\t\t AverageEpisodeLength = 161.5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28999 loss:0.00011"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 673\n",
            "\t\t EnvironmentSteps = 116004\n",
            "\t\t AverageReturn = 1.2000000476837158\n",
            "\t\t AverageEpisodeLength = 190.5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29995 loss:0.00010"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 699\n",
            "\t\t EnvironmentSteps = 120004\n",
            "\t\t AverageReturn = 0.699999988079071\n",
            "\t\t AverageEpisodeLength = 155.39999389648438\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30998 loss:0.00122"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 719\n",
            "\t\t EnvironmentSteps = 124004\n",
            "\t\t AverageReturn = 1.399999976158142\n",
            "\t\t AverageEpisodeLength = 183.6999969482422\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31997 loss:0.00037"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 742\n",
            "\t\t EnvironmentSteps = 128004\n",
            "\t\t AverageReturn = 0.8999999761581421\n",
            "\t\t AverageEpisodeLength = 169.1999969482422\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32998 loss:0.00004"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 764\n",
            "\t\t EnvironmentSteps = 132004\n",
            "\t\t AverageReturn = 1.7000000476837158\n",
            "\t\t AverageEpisodeLength = 180.3000030517578\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "33997 loss:0.00010"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 784\n",
            "\t\t EnvironmentSteps = 136004\n",
            "\t\t AverageReturn = 2.0999999046325684\n",
            "\t\t AverageEpisodeLength = 193.6999969482422\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34999 loss:0.00072"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 799\n",
            "\t\t EnvironmentSteps = 140004\n",
            "\t\t AverageReturn = 3.5\n",
            "\t\t AverageEpisodeLength = 245.89999389648438\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35997 loss:0.00057"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 818\n",
            "\t\t EnvironmentSteps = 144004\n",
            "\t\t AverageReturn = 2.4000000953674316\n",
            "\t\t AverageEpisodeLength = 210.8000030517578\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36996 loss:0.00042"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 835\n",
            "\t\t EnvironmentSteps = 148004\n",
            "\t\t AverageReturn = 3.0999999046325684\n",
            "\t\t AverageEpisodeLength = 210.5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37998 loss:0.00020"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 851\n",
            "\t\t EnvironmentSteps = 152004\n",
            "\t\t AverageReturn = 3.0999999046325684\n",
            "\t\t AverageEpisodeLength = 283.79998779296875\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38998 loss:0.00053"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 861\n",
            "\t\t EnvironmentSteps = 156004\n",
            "\t\t AverageReturn = 5.0\n",
            "\t\t AverageEpisodeLength = 402.3999938964844\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "39999 loss:0.00406"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 873\n",
            "\t\t EnvironmentSteps = 160004\n",
            "\t\t AverageReturn = 4.0\n",
            "\t\t AverageEpisodeLength = 325.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40997 loss:0.00073"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 889\n",
            "\t\t EnvironmentSteps = 164004\n",
            "\t\t AverageReturn = 6.0\n",
            "\t\t AverageEpisodeLength = 268.70001220703125\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "41998 loss:0.00034"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 906\n",
            "\t\t EnvironmentSteps = 168004\n",
            "\t\t AverageReturn = 3.9000000953674316\n",
            "\t\t AverageEpisodeLength = 225.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "42999 loss:0.00050"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 918\n",
            "\t\t EnvironmentSteps = 172004\n",
            "\t\t AverageReturn = 6.699999809265137\n",
            "\t\t AverageEpisodeLength = 355.3999938964844\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "43190 loss:0.00031"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-5d2054b6825c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;31m# Training the agent for 100000 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m \u001b[0mAgentTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-5d2054b6825c>\u001b[0m in \u001b[0;36mAgentTraining\u001b[0;34m(n_iterations)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCollectDriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mtrajectories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlayerAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import suite_atari\n",
        "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
        "from tf_agents.environments.atari_wrappers import FrameStack4\n",
        "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
        "from tf_agents.networks.q_network import QNetwork\n",
        "from tensorflow import keras\n",
        "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "from tf_agents.eval.metric_utils import log_metrics\n",
        "import logging\n",
        "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
        "from tf_agents.trajectories.trajectory import to_transition\n",
        "from tf_agents.utils.common import function\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# animations\n",
        "import matplotlib.animation as animation\n",
        "mpl.rc('animation', html='jshtml')\n",
        "\n",
        "class GameWithAutofire(AtariPreprocessing):\n",
        "    def reset(self, **kwargs):\n",
        "        obs = super().reset(**kwargs)\n",
        "        super().step(1)  # FIRE to start\n",
        "        return obs\n",
        "\n",
        "    def step(self, action):\n",
        "        lives_before_action = self.ale.lives()\n",
        "        obs, rewards, done, info = super().step(action)\n",
        "        if self.ale.lives() < lives_before_action and not done:\n",
        "            super().step(1)  # FIRE to start after life lost\n",
        "        return obs, rewards, done, info\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "environment = suite_gym.load(\"Breakout-v4\")\n",
        "max_episode_steps = 27000  # <=> 108k ALE frames since 1 step = 4 frames\n",
        "environment_name = \"BreakoutNoFrameskip-v4\"\n",
        "\n",
        "# Creating the environment\n",
        "environment = suite_atari.load(\n",
        "    environment_name,\n",
        "    max_episode_steps=max_episode_steps,\n",
        "    gym_env_wrappers=[GameWithAutofire, FrameStack4]) # The game with autofire wrapper is used to restart the game when a life is lost\n",
        "    # Frame stack 4 outputs the observations composed of multiple frames stacked on top of each other along the channels\n",
        "\n",
        "# Wrapping the environment in a TFPyEnvironments wrapper to support tensorflow and python code\n",
        "TensorflowEnv = TFPyEnvironment(environment)\n",
        "\n",
        "# Setting the seed\n",
        "environment.seed(42)\n",
        "environment.reset()\n",
        "\n",
        "def viewUpdate(num, frames, patch):\n",
        "    patch.set_data(frames[num])\n",
        "    return patch,\n",
        "\n",
        "def displayAnimation(frames, repeat=False, interval=40):\n",
        "    fig = plt.figure()\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    anim = animation.FuncAnimation(\n",
        "        fig, viewUpdate, fargs=(frames, patch),\n",
        "        frames=len(frames), repeat=repeat, interval=interval)\n",
        "    plt.close()\n",
        "    return anim\n",
        "\n",
        "def PlotObsFrames(obs):\n",
        "    # Since there are only 3 color channels, you cannot display 4 frames\n",
        "    # with one primary color per frame. So this code computes the delta between\n",
        "    # the current frame and the mean of the other frames, and it adds this delta\n",
        "    # to the red and blue channels to get a pink color for the current frame.\n",
        "    obs = obs.astype(np.float32)\n",
        "    img = obs[..., :3]\n",
        "    current_frame_delta = np.maximum(obs[..., 3] - obs[..., :3].mean(axis=-1), 0.)\n",
        "    img[..., 0] += current_frame_delta\n",
        "    img[..., 2] += current_frame_delta\n",
        "    img = np.clip(img / 150, 0, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "\n",
        "def GetNewScene(num, frames, patch):\n",
        "    patch.set_data(frames[num])\n",
        "    return patch,\n",
        "\n",
        "\n",
        "'''\n",
        "Creating the Deep Q-Network using the tf_agents library\n",
        "'''\n",
        "# Preprocessing layer to normalize the inputs\n",
        "PreprocessLayer = keras.layers.Lambda(lambda obs: tf.cast(obs, np.float32) / 255.)\n",
        "# 3 convolutional layers\n",
        "# Layer1 : 32 filters, filter size = (8,8), stride = 4\n",
        "# Layer2 : 64 filters, filter size = (4,4), stride = 2\n",
        "# Layer1 : 64 filters, filter size = (3,3), stride =\n",
        "ConvLayerHyperparams = [(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\n",
        "# Fully connected layer\n",
        "DenseLayerParams = [550]\n",
        "\n",
        "# Online model\n",
        "DQNetworkModel = QNetwork(\n",
        "    TensorflowEnv.observation_spec(),\n",
        "    TensorflowEnv.action_spec(),\n",
        "    preprocessing_layers=PreprocessLayer,\n",
        "    conv_layer_params=ConvLayerHyperparams,\n",
        "    fc_layer_params=DenseLayerParams)\n",
        "\n",
        "# Target model\n",
        "TargetModel = QNetwork(\n",
        "    TensorflowEnv.observation_spec(),\n",
        "    TensorflowEnv.action_spec(),\n",
        "    preprocessing_layers=PreprocessLayer,\n",
        "    conv_layer_params=ConvLayerHyperparams,\n",
        "    fc_layer_params=DenseLayerParams)\n",
        "\n",
        "    \n",
        "'''\n",
        "Creating the agent to play the game\n",
        "'''\n",
        "TrainingStep = tf.Variable(0)\n",
        "UpdatePeriod = 4  # run a training step every 4 collect steps\n",
        "optimizer = keras.optimizers.RMSprop(learning_rate=2.5e-4, rho=0.95, momentum=0.0,\n",
        "                                     epsilon=0.00001, centered=True)\n",
        "# Using a scheduler to reduce the epsilon value during training\n",
        "EpsFunction = keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=1.0,  # initial epsilon\n",
        "    decay_steps=250000 // UpdatePeriod,  # <=> 1,000,000 ALE frames\n",
        "    end_learning_rate=0.01)  # final epsilon\n",
        "\n",
        "# Defining the agent parameters\n",
        "PlayerAgent = DqnAgent(TensorflowEnv.time_step_spec(), # Time step specifications of the env\n",
        "                       TensorflowEnv.action_spec(), # env Action specifications\n",
        "                       q_network=DQNetworkModel, # DQN model defined earlier\n",
        "                       optimizer=optimizer, # OPtimizer for the DQN\n",
        "                       target_q_network=TargetModel, # Target model for predictions\n",
        "                       target_update_period=2000,  # <=> 32,000 ALE frames\n",
        "                       td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"), # loss fn for the DQN\n",
        "                       gamma=0.99,  # discount factor gamma\n",
        "                       train_step_counter=TrainingStep, # Current training step initialized to 0 as it indicates start of the game\n",
        "                       epsilon_greedy=lambda: EpsFunction(TrainingStep)) # Epsilon value updated by the scheduler which reduces the eps as timestep increases\n",
        "# Initialize the agent\n",
        "PlayerAgent.initialize()\n",
        "\n",
        "'''\n",
        "Create the memory buffer to store all experiences\n",
        "'''\n",
        "MemoryBuffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=PlayerAgent.collect_data_spec, # Data that will be saved in the buffer\n",
        "    batch_size=TensorflowEnv.batch_size, # Number of trajectories that will be stored at each step\n",
        "    max_length=10000)  # reduce if OOM error\n",
        "\n",
        "# Initialize the observer\n",
        "Observer = MemoryBuffer.add_batch\n",
        "\n",
        "\n",
        "class WarmingUp:\n",
        "    def __init__(self, total):\n",
        "        self.counter = 0\n",
        "        self.total = total\n",
        "\n",
        "    def __call__(self, trajectory):\n",
        "        if not trajectory.is_boundary():\n",
        "            self.counter += 1\n",
        "        if self.counter % 100 == 0:\n",
        "            print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")\n",
        "\n",
        "# Defining the metrics\n",
        "TrainingMetrics = [\n",
        "    tf_metrics.NumberOfEpisodes(),\n",
        "    tf_metrics.EnvironmentSteps(),\n",
        "    tf_metrics.AverageReturnMetric(),\n",
        "    tf_metrics.AverageEpisodeLengthMetric(),\n",
        "]\n",
        "# saving the metrics\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "log_metrics(TrainingMetrics)\n",
        "\n",
        "'''\n",
        "Defining the collect driver\n",
        "'''\n",
        "CollectDriver = DynamicStepDriver(\n",
        "    TensorflowEnv, # Environment\n",
        "    PlayerAgent.collect_policy,\n",
        "    observers=[Observer] + TrainingMetrics,\n",
        "    num_steps=UpdatePeriod)  # collect 4 steps for each training iteration\n",
        "\n",
        "initial_collect_policy = RandomTFPolicy(TensorflowEnv.time_step_spec(),\n",
        "                                        TensorflowEnv.action_spec())\n",
        "InitializeDriver = DynamicStepDriver(\n",
        "    TensorflowEnv,\n",
        "    initial_collect_policy,\n",
        "    observers=[MemoryBuffer.add_batch, WarmingUp(10000)],\n",
        "    num_steps=10000)  # <=> 80,000 ALE frames\n",
        "FinalTimeStep, FinalPolicyState = InitializeDriver.run()\n",
        "\n",
        "tf.random.set_seed(9)  # chosen to show an example of trajectory at the end of an episode\n",
        "\n",
        "'''\n",
        "Creating the dataset\n",
        "'''\n",
        "trajectories, buffer_info = next(iter(MemoryBuffer.as_dataset(\n",
        "    sample_batch_size=2,\n",
        "    num_steps=3,\n",
        "    single_deterministic_pass=False)))\n",
        "\n",
        "timeSteps, actionSteps, nextTimeSteps = to_transition(trajectories)\n",
        "dataset = MemoryBuffer.as_dataset(\n",
        "    sample_batch_size=64,\n",
        "    num_steps=2,\n",
        "    num_parallel_calls=3).prefetch(3)\n",
        "\n",
        "CollectDriver.run = function(CollectDriver.run)\n",
        "PlayerAgent.train = function(PlayerAgent.train)\n",
        "\n",
        "'''\n",
        "Training function\n",
        "'''\n",
        "def AgentTraining(n_iterations):\n",
        "    time_step = None\n",
        "    policy_state = PlayerAgent.collect_policy.get_initial_state(TensorflowEnv.batch_size)\n",
        "    iterator = iter(dataset)\n",
        "    for iteration in range(n_iterations):\n",
        "        time_step, policy_state = CollectDriver.run(time_step, policy_state)\n",
        "        trajectories, buffer_info = next(iterator)\n",
        "        train_loss = PlayerAgent.train(trajectories)\n",
        "        print(\"\\r{} loss:{:.5f}\".format(iteration, train_loss.loss.numpy()), end=\"\")\n",
        "        # Display metrics every 1000 iterations\n",
        "        if iteration % 1000 == 0:\n",
        "            log_metrics(TrainingMetrics)\n",
        "\n",
        "\n",
        "# Training the agent for 100000 iterations\n",
        "AgentTraining(n_iterations=100000)\n",
        "\n",
        "\n",
        "'''\n",
        "Rendering after training\n",
        "'''\n",
        "frames = []\n",
        "def StoreFrames(trajectory):\n",
        "    global frames\n",
        "    frames.append(TensorflowEnv.pyenv.envs[0].render(mode=\"rgb_array\"))\n",
        "\n",
        "WatchDriver = DynamicStepDriver(\n",
        "    TensorflowEnv,\n",
        "    PlayerAgent.policy,\n",
        "    observers=[StoreFrames, WarmingUp(1000)],\n",
        "    num_steps=1000)\n",
        "FinaltimeStep, FinalPolicyState = WatchDriver.run()\n",
        "\n",
        "displayAnimation(frames)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtjdK_nKTLky"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CA2 RL final",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
